{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15526,"status":"ok","timestamp":1639590330147,"user":{"displayName":"Ananya Hari-Narain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11555601512454816461"},"user_tz":0},"id":"CyHj-tOcq5Ss","outputId":"77caa0e9-f433-491e-bef4-7df0c5e85629"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9232,"status":"ok","timestamp":1639590339376,"user":{"displayName":"Ananya Hari-Narain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11555601512454816461"},"user_tz":0},"id":"96D_jJHqqEu_","outputId":"baa6f739-6d6a-478f-ad88-52796d4fd7a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.14.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 474 kB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 34.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 38.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 45.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":[""],"metadata":{"id":"3YIooJEDZLz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":210},"id":"JvDbqbttp5gT","outputId":"6ff6b17f-9bf7-411e-fbea-f0555ef65300"},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (7780 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["from transformers import BartTokenizer, BartForConditionalGeneration\n","import pandas as pd\n","import numpy as np\n","\n","\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","df = pd.read_csv(\"/content/drive/MyDrive/Dissertation/data/pickled_for_colab.csv\")\n","#print(df.columns)\n","\n","\n","trans_lst = df.transcript.tolist()\n","\n","encoded_final_lst = []\n","\n","for trans in trans_lst:\n","  enc_lst = tokenizer.encode(trans)\n","  if len(enc_lst) > 512:\n","    encoded_final_lst.append([enc_lst[:256].extend(enc_lst[256:])])\n","\n","#trans = tokenizer.decode(encoded2.ids)\n","\n","\n","#for i in df.iterrows():\n","#    _, transcript = i\n","#    print(\"ep\", ep_id)\n","#    print(\"trans\", transcript)\n","#    break\n","#    encoded = tokenizer.encode(transcript)\n","#    if len(encoded) > 512: #max for bart\n","#        encoded = encoded[:256].extend(encoded[256:])\n","#    trans = tokenizer.decode(encoded.ids)\n","#    lst.append(  [ep_id, trans, descript])"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Truncation.ipynb","provenance":[],"authorship_tag":"ABX9TyMme0aRBIKSuZmGWUDMzIoU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
