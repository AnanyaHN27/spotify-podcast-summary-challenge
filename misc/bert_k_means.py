# -*- coding: utf-8 -*-
"""bert_k_means.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRGKow1WRjr6-m3z5y67PRH7BijUqVqG
"""

!pip install transformers

from nltk.tokenize import sent_tokenize
import torch
from torch.utils.data import TensorDataset, random_split
from transformers import BertTokenizer, BertModel
from transformers import get_linear_schedule_with_warmup
from sklearn.cluster import KMeans
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import numpy as np
import random

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def bert_sent_embedding(sentences, ep_descriptions):
    """
    Input: a list of sentence tokens
    Output: a list of vectors that represent the sentences
    """

    ## tokenisation 
    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
    bert_model = BertModel.from_pretrained('bert-base-cased')

    # Tokenize all of the sentences and map the tokens to thier word IDs.
    input_ids = []
    attention_masks = []
    labels = []
    label_masks = []

    # For every sentence...
    for sent in sentences:
        encoded_dict = tokenizer.encode_plus(
                            sent,                      
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 64,           # Pad & truncate all sentences.
                            pad_to_max_length = True,
                            return_attention_mask = True,  
                            return_tensors = 'pt',     
                      )
        
        # Add the encoded sentence to the list.    
        input_ids.append(encoded_dict['input_ids'])
        
        # And its attention mask (simply differentiates padding from non-padding).
        attention_masks.append(encoded_dict['attention_mask'])

    for sent in ep_descriptions:
        encoded_dict = tokenizer.encode_plus(
                            sent,                      
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 64,           # Pad & truncate all sentences.
                            pad_to_max_length = True,
                            return_attention_mask = True,  
                            return_tensors = 'pt',     
                      )
        
        # Add the encoded sentence to the list.    
        labels.append(encoded_dict['input_ids'])
    
    # Convert the lists into tensors.
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    labels = torch.cat(labels, dim=0)
    label_masks = torch.cat(label_masks, dim=0)

    train_dataset, val_dataset = train_val_split(input_ids, attention_masks, labels)
    train_dataloader, val_dataloader = iterator_batches(train_dataset, val_dataset)
    model = train(train_dataloader, val_dataloader, bert_model)

    with torch.no_grad():
      output = model(**encoded_inp)

    sentence_embedding_list = mean_pooling(output, encoded_inp['attention_mask'])

    return sentence_embedding_list

def k_means(sentence_embedding_list):
  """
  Input: array of sentence embeddings

  Output: ndarray of sentences that are closest to the clusters
  """
  clusters = int(np.ceil(len(sentence_embedding_list)**0.5))
  kmeans = KMeans(n_clusters=clusters).fit(sentence_embedding_list)
  #minimum distances from each sentence cluster to each sentence in the embedding list
  sum_index, _ = sorted(pairwise_distances_argmin_min(kmeans.cluster_centers_, sentence_embedding_list,metric='euclidean'))
  
  return sum_index

def bert_summarise(text):
  
  sentences = sent_tokenize(text)
  sentence_embedding_list = bert_sent_embedding(sentences)
  sum_index = k_means(sentence_embedding_list)
    
  summary = ' '.join([sentences[ind] for ind in sum_index])
    
  return summary

def train_val_split(input_ids, attention_masks, labels):
  dataset = TensorDataset(input_ids, attention_masks, labels)

  #90:10 train:validation split

  train_size = int(0.9 * len(dataset))
  val_size = len(dataset) - train_size
  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
  return train_dataset, val_dataset

def iterator_batches(train_dataset, val_dataset):
  batch_size = 32

  # Create the DataLoaders for our training and validation sets.
  # We'll take training samples in random order. 
  train_dataloader = DataLoader(
              train_dataset,  # The training samples.
              sampler = RandomSampler(train_dataset), # Select batches randomly
              batch_size = batch_size # Trains with this batch size.
          )

  # For validation the order doesn't matter, so we'll just read them sequentially.
  val_dataloader = DataLoader(
              val_dataset, # The validation samples.
              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.
              batch_size = batch_size # Evaluate with this batch size.
          )
  
  return train_dataloader, val_dataloader

def train(train_dataloader, val_dataloader, model):
  optimizer = AdamW(model.parameters(),
                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5
                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.
                )
  epochs = 4
  total_steps = len(train_dataloader) * epochs
  scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 0, # Default value in run_glue.py
                                            num_training_steps = total_steps)
  seed_val = 42

  random.seed(seed_val)
  np.random.seed(seed_val)
  torch.manual_seed(seed_val)
  torch.cuda.manual_seed_all(seed_val)

  training_stats = []

  
  for epoch_i in range(0, epochs):
      
    
      print("")
      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
      print('Training...')

      # Reset the total loss for this epoch.
      total_train_loss = 0

      model.train()

      # For each batch of training data...
      for step, batch in enumerate(train_dataloader):

          b_input_ids = batch[0].to(device)
          b_input_mask = batch[1].to(device)
          b_labels = batch[2].to(device)
          
          model.zero_grad()        

          loss, logits = model(b_input_ids, 
                              token_type_ids=None, 
                              attention_mask=b_input_mask, 
                              labels=b_labels)

          total_train_loss += loss.item()

          # Perform a backward pass to calculate the gradients.
          loss.backward()

          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

          optimizer.step()

          scheduler.step()

      # Calculate the average loss over all of the batches.
      avg_train_loss = total_train_loss / len(train_dataloader)            
      
      print("")
      print("  Average training loss: {0:.2f}".format(avg_train_loss))
     
      print("Running Validation...")

      model.eval()

      # Tracking variables 
      total_eval_accuracy = 0
      total_eval_loss = 0
      nb_eval_steps = 0

      # Evaluate data for one epoch
      for batch in val_dataloader:
          
          b_input_ids = batch[0].to(device)
          b_input_mask = batch[1].to(device)
          b_labels = batch[2].to(device)
          
          with torch.no_grad():        

              (loss, logits) = model(b_input_ids, 
                                    token_type_ids=None, 
                                    attention_mask=b_input_mask,
                                    labels=b_labels)
              
          # Accumulate the validation loss.
          total_eval_loss += loss.item()

          # Move logits and labels to CPU
          logits = logits.detach().cpu().numpy()
          label_ids = b_labels.to('cpu').numpy()

          # Calculate the accuracy for this batch of test sentences, and
          # accumulate it over all batches.
          preds_flatten = np.argmax(logits, axis=1).flatten()
          labels_flatten = label_ids.flatten()
          total_eval_accuracy += np.sum(preds_flatten == labels_flatten) / len(labels_flatten)
          

      # Report the final accuracy for this validation run.
      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
      print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

      # Calculate the average loss over all of the batches.
      avg_val_loss = total_eval_loss / len(validation_dataloader)
      
      # Measure how long the validation run took.
      validation_time = format_time(time.time() - t0)
      
      print("  Validation Loss: {0:.2f}".format(avg_val_loss))


      # Record all statistics from this epoch.
      training_stats.append(
          {
              'epoch': epoch_i + 1,
              'Training Loss': avg_train_loss,
              'Valid. Loss': avg_val_loss,
              'Valid. Accur.': avg_val_accuracy,
          }
      )

  print("")
  print("Training complete!")
  return model

if __name__ == "__main__":
  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pickled_for_colab.csv')
  bert_sent_embedding(df['transcript'], df['episode_description'])
