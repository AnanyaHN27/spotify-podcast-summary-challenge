# -*- coding: utf-8 -*-
"""longformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1alFNfhbTJOX6-LNSkBEi9CwXSdzeSVTX
"""

!pip install transformers
!pip install nlp
!pip install datasets
!pip install rouge_score rouge_score

"""*Imports*"""

import nlp
import logging
from transformers import LongformerTokenizer, EncoderDecoderModel, Trainer, TrainingArguments
from datasets import load_dataset
import torch
import pandas as pd

"""*Setting configurations, loading data*"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

logging.basicConfig(level=logging.INFO)

model = EncoderDecoderModel.from_pretrained("patrickvonplaten/longformer2roberta-cnn_dailymail-fp16")
model.to(DEVICE)
tokenizer = LongformerTokenizer.from_pretrained("allenai/longformer-base-4096")

train_dataset = load_dataset('csv', data_files='/content/drive/MyDrive/Colab Notebooks/pickled_for_colab.csv', split="train[:75%]")
val_dataset = load_dataset('csv', data_files='/content/drive/MyDrive/Colab Notebooks/pickled_for_colab.csv', split="train[:15%]")
test_dataset = load_dataset('csv', data_files='/content/drive/MyDrive/Colab Notebooks/pickled_for_colab.csv', split="train[:10%]")

rouge = nlp.load_metric("rouge", experiment_id=2)

ENCODER_LENGTH = 4096
DECODER_LENGTH = 144
BATCH_SIZE = 16

"""*Decoding and training by instantiating the Trainer*"""

def remove_by_indices(descr, indxs):
  return [e for i, e in enumerate(descr) if i not in indxs]

class DataHandler():

    def __init__(self, dataframe, tokenizer):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.episode_description = self.data.episode_description
        self.transcript = self.data.transcript

    def __len__(self):
        return len(self.episode_description)

    def __getitem__(self, index):

        _transcript = self.episode_description.tolist()
        _description = self.transcript.tolist()

        indxs = []

        for i, des in enumerate(_transcript):
          if not isinstance(des, str):
            indxs.append(i)

        for i, des in enumerate(_description):
          if not isinstance(des, str):
            indxs.append(i)

        episode_description = remove_by_indices(_transcript, indxs)
        transcript = remove_by_indices(_description, indxs)

        transcript = str(self.transcript[index])
        transcript = ' '.join(transcript.split())

        episode_description = str(self.episode_description[index])
        episode_description = ' '.join(episode_description.split())

        source = self.tokenizer.batch_encode_plus([transcript], add_special_tokens= True, max_length= ENCODER_LENGTH, 
                                                  padding='longest', return_tensors='pt')
        target = self.tokenizer.batch_encode_plus([episode_description], max_length= DECODER_LENGTH, 
                                                  padding='longest', return_tensors='pt')

        source_ids = source['input_ids'].squeeze()
        source_mask = source['attention_mask'].squeeze()
        target_ids = target['input_ids'].squeeze()
        target_mask = target['attention_mask'].squeeze()
        global_mask = [[1 if i < 128 else 0 for i in range(seq_len)] for seq_len in len(source_ids) * [ENCODER_LENGTH]]

        batch_of_inputs = {
            'input_ids': source_ids.to(dtype=torch.int64), 
            "global_attention_mask" : global_mask,
            'attention_mask': source_mask.to(dtype=torch.int64), 
            'decoder_input_ids': target_ids.to(dtype=torch.int64),
            'decoder_attention_mask' : target_mask.to(dtype=torch.int64),
            'labels': target_ids.to(dtype=torch.int64)
        }
        return batch_of_inputs


df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/pickled_for_colab.csv",encoding='latin-1')

train_dataset=df.sample(frac=0.7,random_state=42).reset_index(drop=True)
val_dataset=df.drop(train_dataset.index).reset_index(drop=True)

training_set_batch = DataHandler(train_dataset, tokenizer)
val_set_batch = DataHandler(val_dataset, tokenizer)

def metrics(text):
    labels_ids = text.label_ids
    pred_ids = text.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    labels_ids[labels_ids == -100] = tokenizer.eos_token_id

    rouge1 = rouge.compute(predictions=pred_str, references=tokenizer.batch_decode(labels_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True), rouge_types=["rouge1"])["rouge1"].mid
    rouge2 = rouge.compute(predictions=pred_str, references=tokenizer.batch_decode(labels_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True), rouge_types=["rouge2"])["rouge2"].mid
    rougel = rouge.compute(predictions=pred_str, references=tokenizer.batch_decode(labels_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True), rouge_types=["rougel"])["rougel"].mid

    return {
        "rouge1_fmeasure": round(rouge1.fmeasure, 2),
        "rouge2_fmeasure": round(rouge2.fmeasure, 2),
        "rougef_fmeasure": round(rougel.fmeasure, 2),
    }

train_set = load_dataset('csv', data_files='/content/drive/MyDrive/Colab Notebooks/cleaned_up_for_colab.csv', split="train[:75%]")
val_set = load_dataset('csv', data_files='/content/drive/MyDrive/Colab Notebooks/cleaned_up_for_colab.csv', split="train[:25%]")

train_dataset = Dataset.from_dict(training_set_batch[1])
train_dataset.set_format(
    type="torch", columns=["input_ids", "global_attention_mask", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],)

val_dataset = Dataset.from_dict(val_set_batch[1])
val_dataset.set_format(
    type="torch", columns=["input_ids", "global_attention_mask","attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],)

training_args = TrainingArguments(
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    predict_from_generate=True,
    evaluate_during_training=True,
    do_train=True,
    do_eval=True,
    logging_steps=100,
    save_steps=100,
    eval_steps=100,
    overwrite_output_dir=True,
    warmup_steps=200,
    save_total_limit=5,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=metrics,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

"""*Generate summaries*"""

def generate(batch):
    inputs = tokenizer(batch["source"], padding="max_length", truncation=True, max_length=ENCODER_LENGTH, return_tensors="pt")
    y = tokenizer(batch["target"], padding="max_length", truncation=True, max_length=ENCODER_LENGTH, return_tensors="pt")
    input_ids = inputs.input_ids.to(DEVICE)
    attention_mask = inputs.attention_mask.to(DEVICE)

    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=DECODER_LENGTH, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)
    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    targets = tokenizer.batch_decode(y, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    batch["generated"] = output_str
    batch["targets"] = targets
    return batch

results = test_dataset.map(generate, batched=True, batch_size=BATCH_SIZE)

pred_str = results["generated"]
label_str = results["targets"]

pd.DataFrame(results).to_csv('/content/drive/MyDrive/Dissertation/data/longformer_results.csv')
