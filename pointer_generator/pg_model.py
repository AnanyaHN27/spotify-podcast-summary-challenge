# -*- coding: utf-8 -*-
"""pg_model_handle_vocab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FCLJHdMNZ8Vf9O_YzFS1bJ3iQwNfrWSj
"""

!pip install rouge

"""*Imports*"""

import numpy as np
from collections import Counter
import random
from random import shuffle
import torch
import torch.nn as nn
from torch import optim
from torch.nn.utils import clip_grad_norm_
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from rouge import Rouge
import math
from tqdm import tqdm
import pandas as pd
from nltk.tokenize import word_tokenize

"""*Import model code*"""

from model import EncoderRNN, DecoderRNN, Seq2SeqOutput, Seq2Seq
#modified code from ymfa

"""*Setting the device*"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
eps = 1e-31

"""*Vocab and OOV order*"""

class Vocab(object):

  PAD = 0
  SOS = 1
  EOS = 2
  UNK = 3

  def __init__(self):
    self.embeddings = []
    self.word_to_index = {}
    self.word_counter = Counter()
    self.index_to_word = ['<UNK>', '<SOS>', '<EOS>', '<PAD>'] #including reserved words

  def add_to_vocab(self, words):
    for word in words.split(" "):
      if word not in self.word_to_index:
        self.word_to_index[word] = len(self.word_counter)
        self.index_to_word.append(word)
    self.word_counter.update(words)

  def load_embeddings(self, path="/content/drive/MyDrive/Dissertation/data/glove (1).6B.100d.txt"):
    with open(path) as file:
      num_embeds = 0
      for line in file:
        split_line = line.split()
        whole_embed = self.word_to_index.get(line)
        index = whole_embed[0]
        if not index is None:
          self.embeddings[index] = np.float32(np.array(whole_embed[1:]))
        num_embeds += 1
      return num_embeds

  def __len__(self):
    return len(self.word_counter)

class OOVDict(object):

  def __init__(self, base_oov_idx):
    self.word_to_index = {} 
    self.index_to_word = {}  
    self.base_oov_idx = base_oov_idx
    self.ext_vocab_size = 0
    self.oov_counter = Counter()

  def add_word(self, word):
    index = self.word_to_index.get(word, default=None)
    if index is None: 
      index = len(self.oov_counter) + self.base_oov_idx
      self.word_to_index[word] = index
      self.index_to_word[index] = word
      self.ext_vocab_size = len(self.oov_counter) 
    self.oov_counter.update()
    return index

"""*Handling the dataset*"""

class Dataset(object):
  
  def __init__(self, filename):
    self.filename = filename
    self.pairs = []
    self.src_len = 0
    self.tgt_len = 0
    self.word_to_index = {}

    df = pd.read_csv(self.filename) 

    self.transcript = df.transcript
    self.episode_description = df.episode_description
    
    _transcript = self.episode_description.tolist()
    _description = self.transcript.tolist()

    indxs = []

    for i, des in enumerate(_transcript):
      if not isinstance(des, str):
        indxs.append(i)

    for i, des in enumerate(_description):
      if not isinstance(des, str):
        indxs.append(i)

    self.episode_description = self.remove_by_indices(_transcript, indxs)
    self.episode_description = [word_tokenize(i) for i in self.episode_description]
    self.transcript = self.remove_by_indices(_description, indxs)
    self.transcript = [word_tokenize(i) for i in self.transcript]
    self.pairs = list(zip(self.transcript, self.episode_description))

    self.src_len = len(self.transcript) + 1 
    self.tgt_len = len(self.episode_description) + 1 
  
  def remove_by_indices(self, descr, indxs):
    return [e for i, e in enumerate(descr) if i not in indxs]

  def vocab_builder(self, vocab=True,
                  embed_file="/content/drive/MyDrive/Dissertation/data/glove (1).6B.100d.txt"):
    
    vocab = Vocab()
    vocab.embeddings = load_embeddings(path="/content/drive/MyDrive/Dissertation/data/glove (1).6B.100d.txt")
    for pair in self.pairs:
      vocab.add_to_vocab(pair[0]) 
      vocab.add_to_vocab(pair[1])
  
    vocab.index_to_word = filter(lambda word: len(word) > 1, vocab.index_to_word)
    vocab.word_to_index = {word: vocab.word_to_index[word] for word in vocab.word_to_index.keys() if len(word) > 1}

    return vocab

  def create_batch(self, batch_size, vocab):
    
    random.shuffle(self.pairs)
    batch_point = 0

    while True:
      
      examples = self.pairs[batch_point:batch_point + batch_size]
      batch_point += batch_size
      
      max_src_len = max(len(pair[0]) for pair in examples)
      src_tensor = torch.zeros(max_src_len, batch_size, dtype=torch.long)
      max_tgt_len = max(len(pair[1]) for pair in examples)
      tgt_tensor = torch.zeros(max_tgt_len, batch_size, dtype=torch.long)

      oov_dict = OOVDict(len(vocab))
    
      for i, example in enumerate(examples):
        for pair in range(2):
          for j, word in enumerate(example[pair]):
            if pair == 0:
              eos_pos = len(example[pair]) - 1
              src_tensor[eos_pos,i] = vocab.EOS
              idx = vocab.word_to_index.get(word, default = self.UNK)
              if idx == vocab.UNK:
                idx = oov_dict.add_word(word)
              src_tensor[j, i] = idx
            else:
              eos_pos = len(example[pair]) - 1
              tgt_tensor[eos_pos,i] = vocab.EOS
              idx = tgt_vocab.word_to_index.get(word, default = self.UNK)
              if idx == vocab.UNK:
                idx = oov_dict.add_word(word)
              tgt_tensor[j, i] = idx

      batch = {
          "examples": examples,
          "input": src_tensor,
          "target": tgt_tensor,
          "inp_lengths": lengths,
          "oov_dict": oov_dict
      }
      yield batch

"""*Code for training*"""

def train_batch(batch, model, criterion, optimizer,
                forcing_ratio=0.5, partial_forcing=True,
                rl_ratio=0.5, vocab=None, show_cover_loss=False):
  input_lengths = batch["inp_lengths"]

  optimizer.zero_grad()
  input_tensor = batch["input"].to(DEVICE)
  target_tensor = batch["target"].to(DEVICE)
  ext_vocab_size = batch["oov_dict"].ext_vocab_size

  out = model(input_tensor, target_tensor, input_lengths, criterion,
              forcing_ratio=forcing_ratio, partial_forcing=partial_forcing, sample=True,
              ext_vocab_size=ext_vocab_size, include_cover_loss=show_cover_loss)

  if rl_ratio>0:
    samp = model(input_tensor, saved_out=out, criterion=criterion, sample=True,
                       ext_vocab_size=ext_vocab_size)
    #this is a randomly sampled output
    baseline = model(input_tensor, saved_out=out, ext_vocab_size=ext_vocab_size)
    #this is the greedy baseline generated by the model
    targets_from_baseline_sample = [ex.tgt for ex in batch["examples"]]
    scores = eval_batch_output(targets_from_baseline_sample, vocab, batch["oov_dict"],
                               samp.decoded_tokens, baseline.decoded_tokens)
    greedy_rouge = scores[1]['f']
    neg_reward = greedy_rouge - scores[0]['f']
    # if sample > baseline, the reward is positive and rl_loss is negative
    rl_loss = neg_reward * samp.loss
    rl_loss_value = neg_reward * samp.loss_value
    loss = (1 - rl_ratio) * out.loss + rl_ratio * rl_loss
    loss_value = (1 - rl_ratio) * out.loss_value + rl_ratio * rl_loss_value
  else:
    loss = out.loss
    loss_value = out.loss_value

  loss.backward()
  optimizer.step()

  return loss_value / target_tensor.size(0)

def train(train_generator, vocab, model, valid_generator=None, rl_ratio=1, saved_state=None):
  
  model.to(DEVICE)
  
  optimizer = optim.Adagrad(model.parameters(), lr=0.001, initial_accumulator_value=0.1)
    
  past_epochs = 10
  rl_start_epoch = 2
  total_batch_count = 0
  
  for epoch_count in range(1 + past_epochs,  100):
    if epoch_count >= rl_start_epoch:
      rl_ratio = 1
    else:
      rl_ratio = 0
    
    prog_bar = tqdm(range(1, 1000 + 1), desc='Epoch %d' % epoch_count)
    model.train()

    for batch_count in prog_bar:  # training batches
     
      forcing_ratio = 0.75  * 0.9999 / (
                  0.9999 + np.exp(total_batch_count / 0.9999))

      batch = next(train_generator)
      loss = train_batch(batch, model, criterion=nn.NLLLoss(ignore_index=vocab.PAD), optimizer,
                                 forcing_ratio=forcing_ratio,
                                 partial_forcing=True,
                                 vocab=vocab,
                                 show_cover_loss=False)

      epoch_loss += float(loss)
      epoch_avg_loss = epoch_loss / batch_count
      
      prog_bar.set_postfix(loss='%g' % epoch_avg_loss)

      total_batch_count += 1

    if rl_ratio > 0:
      rl_ratio **= rl_ratio_power

    filename = '%s.%0d.pt' % ("/content/drive/MyDrive/Dissertation/data/afresh", epoch_count)
    torch.save(model, filename)

    for epoch_id in range(1 + past_epochs, epoch_count):
      # save training status
      torch.save({
        'epoch': epoch_count,
        'total_batch_count': total_batch_count,
        'train_avg_loss': epoch_avg_loss,
        'valid_avg_loss': valid_avg_loss,
        'valid_avg_metric': valid_avg_metric,
        'best_epoch_so_far': best_epoch_id,
        'optimizer': optimizer
      }, '%s.train.pt' % "model_path_prefix")

"""*Creating the hypothesis for potential summaries*"""

class Hypothesis(object):

  def __init__(self, tokens, log_probs, dec_hidden, dec_states, attn_dists, coverage):
    self.tokens = tokens  
    self.log_probs = log_probs  
    self.dec_hidden = dec_hidden  
    self.dec_states = dec_states  
    self.attn_dists  = attn_dists   
    self.coverage = coverage

  @property
  def log_prob(self):
    return sum(self.log_probs)

  @property
  def avg_log_prob(self):
    return self.log_prob / len(self.tokens)

  def __len__(self):
    return len(self.tokens) - self.num_non_words

  def extend_hypothesis(self, token, log_prob, dec_hidden, attn, p_gen, cov):

    toks = self.tokens[:]
    toks.append(token)
    probs = self.log_probs[:]
    probs.append(log_prob)
    decoder_state = self.dec_states + [dec_hidden]
    attn_dists = self.attn_dists [:]
    attn_dists.append(attn)

    return Hypothesis(tokens= toks, log_probs= probs,
                      dec_hidden=dec_hidden, dec_states=decoder_state,
                      attn_dists =attn_dists, coverage = cov)
    
  def beam_search(self, vocab, input_tensor, input_lengths=None, ext_vocab_size=None, beam_size=10,
                  min_out_len=100, max_out_len=144, beam_batch):
    batch_size = input_tensor.size(1)
    
    encoder_hidden = beam_batch["encoder_hidden"]
    encoder_embedded = beam_batch["encoder_embedded"]
    encoder_outputs = beam_batch["encoder_outputs"]
    decoder_hidden = beam_batch["decoder_hidden"]

    input_tensor = input_tensor.expand(-1, beam_size).contiguous()

    # decode
    hypos = [Hypothesis([self.vocab.SOS], [0.0], decoder_hidden, [], [], [])]
    results= [] 
    steps = 0
    while len(results)<beam_size and steps < 2 * max_out_len:  
      
      latest_tokens = [h.tokens[0] for h in hypos]
      latest_tokens = [tok if tok in range(len(vocab)) else vocab.UNK for tok in latest_tokens]
      decoder_hidden = [h.dec_hidden for h in hypos]
      
      states = [torch.cat(h.dec_states, 0) for h in hypos]
      
      enc_attn_weights = [[h.enc_attn_weights[i] for h in hypos] for i in range(steps)]
      
      coverage_vector = [h.coverage for h in hypos]
      
      # run the decoder over the assembled batch to get new info
      decoder_embedded = self.embedding(self.filter_oov(latest_tokens, ext_vocab_size))
      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \
        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,
                    states, coverage_vector,
                    encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size)
      top_v, top_i = decoder_output.data.topk(beam_size) 
      
      # extend
      all_hypos = []
      
      n_hypos = 1 if steps == 0 else len(hypos)

      for i in range(n_hypos):
        for j in range(beam_size * 2):
          new_tok = top_i[i][j].item()
          new_prob = top_v[i][j].item()
    
          new_hypo = hypos[i].extend_hypothesis(new_tok, new_prob,
                                              decoder_hidden[0][i].unsqueeze(0).unsqueeze(0),
                                              dec_enc_attn[i].unsqueeze(0).unsqueeze(0), 
                                          max(enc_attn_weights))
          all_hypos.append(new_hypo)
      # process the new hypotheses
      hypos = []
      for h in sorted(all_hypos, key=lambda h: h.avg_log_prob, reverse=True):
        if h.tokens[0] == self.vocab.EOS:  
          if len(h) >= min_out_len and len(h) <= max_out_len:
            results.append(h)
        elif len(hypos) < beam_size and len(h) < max_out_len:  
          # incomplete hypotheses
          hypos.append(h)
        if len(hypos) == beam_size or len(results) == beam_size:
          break

      steps += 1
    if len(results) == 0:  
      results = hypos 

    #returns a list of hypotheses sorted by descending average log probability
    return sorted(results, key=lambda h: h.avg_log_prob, reverse=True)[:beam_size]

"""*Code to decode the summaries*"""

def decode_batch_output(decoded_tokens, vocab, oov_dict):
  index_to_word = {value:key for key, value in vocab.word_to_index.items()}
  
  decoded_batch = []
  if not isinstance(decoded_tokens, list):
    decoded_tokens = decoded_tokens.transpose(0, 1).tolist()
  for i, doc in enumerate(decoded_tokens):
    print("doc", doc)
    decoded_doc = []
    for word_idx in doc:
      if word_idx >= len(vocab):
        word = oov_dict.index_to_word.get((i, word_idx), '<UNK>')
      else:
        word = vocab.index_to_word.get(word_idx, default = self.UNK)
      decoded_doc.append(word)
      if word_idx == vocab.EOS:
        break 
        #Stop at end of sentence
    decoded_batch.append(decoded_doc)
  print("\n decoded batch", decoded_batch)

  decoded_batch_out = []
  for doc in decoded_tokens:
    for tok in doc:
      if tok in index_to_word.keys():
        print("the word for index ", tok, " is ", index_to_word[tok])
        decoded_batch_out.append(index_to_word[tok])

  print(decoded_batch_out)
  return decoded_batch_out


def decode_batch(batch, model, vocab, criterion=None, show_cover_loss=False):
  input_lengths = batch["inp_lengths"]
  with torch.no_grad():
    input_tensor = batch["input"].to(DEVICE)
    target_tensor = batch["target"].to(DEVICE)
    out = model(input_tensor, target_tensor, input_lengths, criterion,
                ext_vocab_size=batch.ext_vocab_size, include_cover_loss=show_cover_loss)
    decoded_batch = decode_batch_output(out.decoded_tokens, vocab, batch["oov_dict"])
  target_length = batch["target"].size(0)
  out.loss_value = out.loss_value / target_length
  return decoded_batch, out

def get_rouge_score_cust(preds, actuals, rouge_type):
    rouge = Rouge()
    tot_scores = {'r': 0, 'p': 0, 'f': 0}
    for i, ac in enumerate(actuals):
        scores = rouge.get_scores(preds[i], ac, avg=True)
        for key in scores[rouge_type].keys():
            tot_scores[key] += scores[rouge_type][key]
    for key in tot_scores.keys():
        tot_scores[key] = tot_scores[key]/len(actuals)
    return tot_scores

def rouge(target, *preds):
  scores = []
  for i, targ_eval in enumerate(target):
    scores.append(get_rouge_score_cust(targ_eval, preds[0][i], "rouge-1"))
    scores.append(get_rouge_score_cust(targ_eval, preds[1][i], "rouge-1"))
  return scores

def eval_batch_output(tgt_tensor, vocab, oov_dict, *pred_tensors):
  decoded_batch = [decode_batch_output(pred_tensor, vocab, oov_dict)
                   for pred_tensor in pred_tensors]
  if isinstance(tgt_tensor, torch.Tensor):
    gold_summaries = decode_batch_output(tgt_tensor, vocab, oov_dict)
  else:
    gold_summaries = tgt_tensor
  scores = rouge(gold_summaries, *decoded_batch)
  return scores

def evaluate(test_set, vocab, model):
  test_gen = test_set.create_batch(1, vocab)

  model.eval()
  prog_bar = tqdm(range(1, int(len(test_set.pairs) + 1))

  df = pd.DataFrame(columns = ['predicted', 'episode_description', 'transcript'])

  list_maintain = []

  for i in prog_bar:
    batch = next(test_gen)

    with torch.no_grad():
      input_tensor = batch["input"].to(DEVICE)

      encoder_hidden = model.encoder.init_hidden(batch_size)
      encoder_embedded = model.embedding(model.filter_oov(input_tensor, ext_vocab_size))
      encoder_outputs, encoder_hidden = model.encoder(encoder_embedded, encoder_hidden, None)
      beam_batch = {
          "encoder_hidden": encoder_hidden,
          "encoder_embedded": encoder_embedded,
          "encoder_outputs": encoder_outputs.expand(-1, beam_size=4, -1).contiguous(), 
          "decoder_hidden": model.enc_dec_adapter(encoder_hidden),
      }
      beam_search_obj = Hypothesis([self.vocab.SOS], [0.0], decoder_hidden, [], [], [])
      hypotheses = beam_search_obj.beam_search(input_tensor, vocab, batch["inp_lengths"],
                                    batch["oov_dict"].ext_vocab_size, beam_size=4, min_out_len=60,
                                    max_out_len=144, beam_batch)
      
      to_decode = [h.tokens for h in hypotheses]

      decoded_batch = decode_batch_output(to_decode, vocab, batch["oov_dict"])
      print(batch["examples"][0][1])
      print(batch["examples"][0][0])
      file_content = [format_nicely(decoded_batch[0]), format_nicely(batch["examples"][0][1]), format_nicely(batch["examples"][0][0])]
      list_maintain.append(file_content)
  file_content = pd.DataFrame(list_maintain)
  vertical_concat = pd.concat([df, file_content], axis=0)
  vertical_concat.to_csv("/content/drive/MyDrive/Dissertation/data/pg_results.csv")

"""*Format the scores to write*"""

def format_nicely(tokens):
  tokens = filter(lambda t: t not in {'<PAD>', '<SOS>', '<EOS>', '<UNK>'}, tokens)
  tokens = list(tokens)
  tokens_sorted = ''.join(map(str, tokens))
  return tokens_sorted

"""*Main code to train and generate summaries*"""

if __name__ == "__main__":
  filename = "/content/drive/MyDrive/Dissertation/data/pickled_for_colab.csv"
  dataset = Dataset(filename)
  vocab = dataset.vocab_builder()
  model = Seq2Seq(vocab)
  train_gen = dataset.create_batch(1, vocab)
  dataset = Dataset("/content/drive/MyDrive/Dissertation/data/pickled_for_colab.csv")
  evaluate(dataset, vocab, model)
