# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15GO7ynFp51SYkDbETnFo823SPPm0evza
"""

"""*Imports*"""

import sqlite3
import pandas as pd
import spacy
import numpy as np 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn.svm import SVC
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.pipeline import Pipeline 
from gensim import parsing 
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import TreebankWordTokenizer
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download("stopwords")
nltk.download('wordnet')
import time

start = time.time()

"""*Cleaning and tokenising sentences*"""

def clean_sentence(text):
        stop_words = stopwords.words('english')
        wordnet_lemmatizer = WordNetLemmatizer()
        regex_tokenizer = RegexpTokenizer(r'\w+')
        tokenized_text = regex_tokenizer.tokenize(text)
        tokenized_text = [w.lower() for w in tokenized_text if w.isalpha()]
        tokenized_text = [w for w in tokenized_text if not w in stop_words]
        tokenized_text = [wordnet_lemmatizer.lemmatize(
            w) for w in tokenized_text]
        return tokenized_text

def tokenise(text):
    tokens = []
    sent_text = nltk.sent_tokenize(text)
    for line in sent_text:
        tokens.extend(clean_sentence(line.strip()))
    return ' '.join(str(elem) for elem in tokens)

"""*Different data depending on method used*"""

AUTO_ANNOTATED = True

def hybrid_using_auto_annotated():
        cnx = sqlite3.connect('podcast_test_set.db')
        df = pd.read_sql_query("SELECT * FROM dataset_", cnx)
        ep_id, X, y = df['episode_id'].tolist(), df['transcript'].tolist(), df['sponsor'].tolist()
        return df, ep_id, X, y

#Using the self-annotated test set

def using_self_annotated():
        df_labels = pd.read_csv('labels.csv')
        df_labels = df_labels[(df_labels.labels != "No") | (df_labels.labels != "Yes")]
        df = pd.concat([df, df_labels], axis=1)
        df = df[0: len(df_labels)]
        df = pd.read_csv('tiny_test.csv')
        X, y = df['transcript'].tolist(), df['labels'].tolist()
        return df, ep_id, X, y

if AUTO_ANNOTATED:
    df, ep_id, X, y = hybrid_using_auto_annotated()
else:
    df, ep_id, X, y = using_self_annotated()

X_to_ep = dict(zip(X, ep_id))

for i in range(0,len(df)):
    df.iloc[i,1]=tokenise(df.iloc[i,1])

"""*Getting the training and test splits*"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=11)
X_dev, X_train = train_test_split(X_train, test_size=0.78, random_state=11)

"""*Training *"""

text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf', gamma='scale'))])

print ("Training")
#train model
text_clf.fit(X_train, y_train)

print("Predicting")
#predict class from test data 
predicted = text_clf.predict(X_dev)

end = time.time()

"""*Printing the metrics*"""

print("Macro Precision: ", precision_score(y_test, predicted, average='macro'))
print("Micro Precision: ", precision_score(y_test, predicted, average='micro'))
print("Macro Recall: ", recall_score(y_test, predicted, average='macro'))
print("Micro Recall: ", recall_score(y_test, predicted, average='micro'))
print(end-start)


"""*Writing the sponsored episodes out to the pkl file*"""

def get_sponsored_episodes(predicted, X_test, X_to_ep):
        filter_out = []
        for i, prediction in enumerate(predicted):
            if prediction == "Yes":
                filter_out.append(X_to_ep[X_test[i]])
        return filter_out

filter_out = get_sponsored_episodes(predicted, X_test, X_to_ep)
                
episode_id_list = []
transcript_list = []

for i, prediction in enumerate(predicted):
        if prediction == "No":
                episode_id_list.append(X_to_ep[X_test[i]])
                transcript_list.append(X_test[i])

intermediate_dict = {'episode_id': episode_id_list, 'transcript': transcript_list}
df_intermediate = pd.DataFrame(intermediate_dict)

df_metadata = pd.read_csv('metadata.tsv', sep='\t')
final_bart_data = pd.merge(df_intermediate, df_metadata, on='episode_id', how='inner')
final_bart_data.to_pickle("bart_train_svm.pkl")

df_filter = pd.DataFrame(filter_out)
df.to_csv('eps_to_filter.csv', index=False)
